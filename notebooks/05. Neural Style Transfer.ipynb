{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cathedral-reliance",
   "metadata": {},
   "source": [
    "### Intro ###\n",
    "\n",
    "Neural style transfer is an algorithm that given a reference style image will make another image to take the visual appearance of the reference image. Ever wandered how your selfie would look like if painted by Van Gogh or Picasso?\n",
    "\n",
    "### Modules ###\n",
    "\n",
    "I'd need the `sys` module to get the executable path and install some additional modules, the `time` one to time different events, `tensorflow` cause this is deep learning and we need tensors, `cv2` to process images and `numpy` to operate with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-provider",
   "metadata": {},
   "source": [
    "Two more required modules are the `tensorflow-hub` module which will allow us to load already trained (pretrained) machine learning models and `tflite-runtime` for the same purpose, but for models that are a little more optimized to run on computers with little or no resources, like the Raspberry Pi is.\n",
    "\n",
    "The next command just calls the `python` interpreted associated with the current environment, calls the pip module with `-m pip` and asks it to install the `tensorflow-hub` and `tflite-runtime` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install tensorflow-hub tflite-runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-paint",
   "metadata": {},
   "source": [
    "After the installation is successful, let me load the `tensorflow_hub` module with a simpler name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-victory",
   "metadata": {},
   "source": [
    "### Models ###\n",
    "\n",
    "The neural style transfer model that I'll use is the `arbistrary-image-stylization` model available on [tensorflow hub](https://www.tensorflow.org/tutorials/generative/style_transfer). As tensorflow is a dying technology, I suggest you download the model and load it from Raspberry Pi's storage. The following box will download the model into the current working directory. Line by line, the code actually:\n",
    "- `wget URL -O path` will download the file from `URL` to `path` due to `-O` switch;\n",
    "- `mkdir -p path` will create a folder at `path`, including all its parents due to `-p` switch;\n",
    "- `tar -xzf archive_path -C path` will extract the content from the `tar.gz` archive with the name `archive_path` (if the type of archive is `tar.bz2` then the switch changes to `-xjf`), and copy the content to the `path` folder due to `-C` switch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2?tf-hub-format=compressed\" -O  magenta-arbitrary-image-stylization-v1-256.tar.gz\n",
    "!mkdir -p magenta-arbitrary-image-stylization-v1-256\n",
    "!tar -xzf magenta-arbitrary-image-stylization-v1-256.tar.gz -C magenta-arbitrary-image-stylization-v1-256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-acceptance",
   "metadata": {},
   "source": [
    "Next, I just load the model from the newly created folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('magenta-arbitrary-image-stylization-v1-256')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-objective",
   "metadata": {},
   "source": [
    "Now that the model is taken care of, I can just download two more models that will actually be doing the same thing. I'll skip the explanations for now, but return further in this notedbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite -O arbitrary-image-stylization-v1-256-prediction-int8.tflite\n",
    "!wget https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite -O arbitrary-image-stylization-v1-256-transfer-int8.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-cleanup",
   "metadata": {},
   "source": [
    "### Style Image ###\n",
    "\n",
    "Let me download a style image.\n",
    "How does _Starry Night_ by Van Gogh sound like? Here's the Wikipedia [url](https://upload.wikimedia.org/wikipedia/commons/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg).\n",
    "\n",
    "Or maybe the _Guitariste, La mandoliniste_ by Picasso? Here's the Wikipedia [url](https://upload.wikimedia.org/wikipedia/en/c/ca/Pablo_Picasso%2C_1910-11%2C_Guitariste%2C_La_mandoliniste%2C_Woman_playing_guitar%2C_oil_on_canvas.jpg)\n",
    "\n",
    "Or maybe the _The Kiss_, from Klimt? Here's the Wikipedia [url](https://upload.wikimedia.org/wikipedia/commons/4/40/The_Kiss_-_Gustav_Klimt_-_Google_Cultural_Institute.jpg).\n",
    "\n",
    "As I'm undecided, I'll download all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -O starry_night_van_gogh.jpg\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/The_Kiss_-_Gustav_Klimt_-_Google_Cultural_Institute.jpg/1024px-The_Kiss_-_Gustav_Klimt_-_Google_Cultural_Institute.jpg -O the_kiss_klimt.jpg\n",
    "!wget https://upload.wikimedia.org/wikipedia/en/c/ca/Pablo_Picasso%2C_1910-11%2C_Guitariste%2C_La_mandoliniste%2C_Woman_playing_guitar%2C_oil_on_canvas.jpg -O guitariste_picasso.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-devil",
   "metadata": {},
   "source": [
    "The next step is to prepare the style image so it's a floating point image, with pixels between 0 and 1 (by default, `cv2.imread` will read the image as having integer 8-bit pixels, with values between 0 and 255) by converting to float with `astype(np.float32)` and dividing afterwards with `255.0`.\n",
    "\n",
    "There's a trick there as we need a tensor with 4 dimensions and an image has only 3: using the np.newaxis on the desired position, will add a new dimension to said matrix. So, A[np.newaxis, :] will create a matrix with shape (1, ?) from an original A matrix of shape (?, ). There's another operator used which is called `ellipsis` - the triple dots `...`. In this situation, it means _take all from all other positions_. So A[np.newaxis, ...] is the same as A[np.newaxis, :, :, :] if A is of shape (?, ?, ?). The `tf.image.resize` is very similar to `cv2.resize`, but works well on the 4-dimensional tensor with the image.\n",
    "\n",
    "The style image needs to be resized to 256 by 256 pixels as this is the size of image used to train the neural style transfer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = cv2.imread('starry_night_van_gogh.jpg')\n",
    "style_img = style_img.astype(np.float32)[np.newaxis, ...] / 255.0\n",
    "style_img = tf.image.resize(style_img, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-interference",
   "metadata": {},
   "source": [
    "Let me take a look at the resulting image and for that I'll use `matplotlib.pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-stroke",
   "metadata": {},
   "source": [
    "New things that I use in the following box are:\n",
    "- using `plt.subplots` to actually just set the size of the output graph; the method returns a figure and an axis object; the axis object is subscribable (so if more subgraphs need to be plotted on the same image, they can be easily referenced by number), and also it behaves exactly like the `plt` object;\n",
    "- used the `::-1` on the last dimension, which represent the color channels, and as I've previously mentioned, are usually represented in the BGR format; using `::-1` will just change the order of the channels from BGR to RGB;\n",
    "- again the `...` ellipsis which this time replaces the two remaining arguments;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.imshow(style_img[0, ..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-episode",
   "metadata": {},
   "source": [
    "### Styling ###\n",
    "\n",
    "First, I'll capture an image from the attached camera using `cv2.VideoCapture` to create a camera object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-classics",
   "metadata": {},
   "source": [
    "Because I want to time everything, I will use the `time.time` to record the start and end time. The current camera image is retrieved using the `read` method on the previously defined camera object. The image is converted further to `float` and resize. There is no need for resizing, as the model works with arbistrary sized images, but this will make it run around 4 times faster. A tensorflow hub model will be invoked as a call on the input data, which in this case is the image that needs to be stylized and the image that will provide the style. The output image is the first output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = time.time()\n",
    "_, current_img = camera.read()\n",
    "current_img = current_img.astype(np.float32)[np.newaxis, ...] / 255\n",
    "current_img = tf.image.resize(current_img, (256, 256))\n",
    "outputs = model(tf.constant(current_img), tf.constant(style_img))\n",
    "stylized_image = outputs[0]\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-classroom",
   "metadata": {},
   "source": [
    "Remember to release the camera so you can use it in other scripts as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-imperial",
   "metadata": {},
   "source": [
    "### Results ###\n",
    "\n",
    "Checking the time it took for the inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end_time - begin_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-blank",
   "metadata": {},
   "source": [
    "Pretty long time as the algorithm is quite complex, as discussed previously. Now, to see the results I'm using the full power of `plt.subplots` by providing the first and second arguments which are actually the number of rows and the number of columns, respectively, in which subplots will be organized. In this situation, the axis object becomes indexed so I can access various subplot independently.\n",
    "\n",
    "As my camera is upside down, I used `::-1` on the second and third position of the tensor slice for the output stylized image, as this will actually change the order of the pixels on both the vertical and the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6))\n",
    "_ = ax[0].imshow(style_img[0, ..., ::-1])\n",
    "_ = ax[0].set_title('Style Image')\n",
    "_ = ax[1].imshow(stylized_image[0, ::-1, ::-1, ::-1])\n",
    "_ = ax[1].set_title('Processed Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-slave",
   "metadata": {},
   "source": [
    "Looks pretty good, right?\n",
    "\n",
    "### Improvements ###\n",
    "\n",
    "But still, on pretty small images, the algorithm is very slow. To speed it up, I can actually divide it into two parts: a part that will extract the style data and one that will apply the extracted style data onto a new image. Tensorflow Hub gives two versions of the two parts of the neural style transfer model. I'll use the int8 version as that's more suited for CPU usage, while the float16 version is built with graphics cards in mind.\n",
    "\n",
    "The prediction model is the one that converts the style image to a style data embedding, like the face embeddings that I was discussing in the previous notebook. Tensorflow Lite is actually a runtime that takes pretrained models and allows inference from them. After loading the model, the tensors memory needs to be allocated and with the `get_input_details` and `get_output_details` methods, the information about the portion of memory allocated for each tensor is extracted (under the `index` key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = tf.lite.Interpreter(model_path=\"arbitrary-image-stylization-v1-256-prediction-int8.tflite\")\n",
    "pred_model.allocate_tensors()\n",
    "\n",
    "pred_input_details = pred_model.get_input_details()\n",
    "pred_output_details = pred_model.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-malaysia",
   "metadata": {},
   "source": [
    "Let me check how the input and the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-reform",
   "metadata": {},
   "source": [
    "For the style image, the processing is similar: load image, convert it to `np.float32` and limit the values to the interval between 0 and 1. The size required is specified in the `pred_input_details` as 256 by 256, same as the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = cv2.imread('guitariste_picasso.jpg')\n",
    "style_img = style_img.astype(np.float32)[np.newaxis, ...] / 255\n",
    "style_img = tf.image.resize(style_img, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-certificate",
   "metadata": {},
   "source": [
    "To do inference, the memory of the previously allocated tensor needs to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.set_tensor(pred_input_details[0]['index'], style_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-peeing",
   "metadata": {},
   "source": [
    "Then the model needs to be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-corner",
   "metadata": {},
   "source": [
    "After which, the tensor data corresponding to the output index can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_data = pred_model.get_tensor(pred_output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-memorabilia",
   "metadata": {},
   "source": [
    "### Reusing Style Data ###\n",
    "\n",
    "As usually the style data needs to be loaded only once, the Tensorflow Lite allows reusing blocks of memory allocated to tensors. I'll load the transfer model next and extract the input and output details, as previously done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_model = tf.lite.Interpreter(model_path=\"arbitrary-image-stylization-v1-256-transfer-int8.tflite\")\n",
    "tran_model.allocate_tensors()\n",
    "\n",
    "tran_input_details = tran_model.get_input_details()\n",
    "tran_output_details = tran_model.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-senator",
   "metadata": {},
   "source": [
    "Let me check the inputs and the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_output_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-intervention",
   "metadata": {},
   "source": [
    "As the input for the transfer model is the style data, this can be set only once and reused for converting multiple images afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_model.set_tensor(tran_input_details[1]['index'], style_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-boating",
   "metadata": {},
   "source": [
    "Then, I can just capture some camera data initializing a new `cv2.VideoCapture` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-ozone",
   "metadata": {},
   "source": [
    "The only difference from the previously used code is that I resize the image to 384 by 384 as the input details suggest. The rest is a combination of the first full model and of the invocation from the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = time.time()\n",
    "_, current_img = camera.read()\n",
    "current_img = current_img.astype(np.float32)[np.newaxis, ...] / 255\n",
    "current_img = tf.image.resize(current_img, (384, 384))\n",
    "tran_model.set_tensor(tran_input_details[0]['index'], current_img)\n",
    "tran_model.invoke()\n",
    "stylized_image = tran_model.get_tensor(tran_output_details[0]['index'])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-diversity",
   "metadata": {},
   "source": [
    "I should release the camera so it can be used by others as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-marker",
   "metadata": {},
   "source": [
    "### Faster Results ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-reason",
   "metadata": {},
   "source": [
    "The inference time has decrease almost 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end_time - begin_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-context",
   "metadata": {},
   "source": [
    "And the results look kind of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6))\n",
    "_ = ax[0].imshow(style_img[0, ..., ::-1])\n",
    "_ = ax[0].set_title('Style Image')\n",
    "_ = ax[1].imshow(stylized_image[0, ::-1, ::-1, ::-1])\n",
    "_ = ax[1].set_title('Processed Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-tf2",
   "language": "python",
   "name": "python-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

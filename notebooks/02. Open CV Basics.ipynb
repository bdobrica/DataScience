{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "failing-republic",
   "metadata": {},
   "source": [
    "## Intro ##\n",
    "\n",
    "While the focus of this notebook is to detect a face in a picture, I'll also try to cover some basics, to get a better understanding of Python.\n",
    "\n",
    "### Running Terminal Commands ###\n",
    "\n",
    "This Jupyter Notebook is so cool that allows you to run terminal commands directly in its cells. To run a command, just open a new cell and put a `!` (exclamation mark) just in front of the row, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bogdan\n",
    "!touch bogdan/dobrica.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-casting",
   "metadata": {},
   "source": [
    "- The first command has creates a folder named `bogdan` in same folder as this notebook, as `mkdir` comes from `make directory` (directory = folder).\n",
    "- The second command created an empty python script file, named `dobrica.py` inside the `bogdan` folder. Actually, the `touch` command means *change the date & time of the file passed as argument, but if the file doesn't exist create a new one*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-smell",
   "metadata": {},
   "source": [
    "### Modules ###\n",
    "\n",
    "Modules are pieces of software written by others that we can reuse and do things the lazy way. Actually, a module is usually a folder that contains some python scripts. Let me show you. Remember the previous folder and file we've just created? You can use them as (useless) modules in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bogdan import dobrica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-elizabeth",
   "metadata": {},
   "source": [
    "You can check if the module is loaded correctly using the `dir` function applied directly on the name of the module. It will show whatever the module contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dobrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-malta",
   "metadata": {},
   "source": [
    "Whoa! Even useless modules have things inside. Those are put there by default and you can access them with dots, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "dobrica.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-oxford",
   "metadata": {},
   "source": [
    "Last, let's make the `dobrica` module do something. Let's edit the file and add a variable inside. You can use the Jupyter Notebook editor for adding this line in `dobrica.py` file:\n",
    "```python\n",
    "is_doing = 'good'\n",
    "```\n",
    "Unfortunately, modules are cached. So to see if the change works, now it's the time to restart the `Kernel` in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bogdan import dobrica\n",
    "dir(dobrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-founder",
   "metadata": {},
   "source": [
    "Already `is_doing` appears in the list. You can call it by using the dot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "dobrica.is_doing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-speech",
   "metadata": {},
   "source": [
    "### Some Programming Basics ###\n",
    "\n",
    "Any computer program can be written knowing just a few simple things.\n",
    "\n",
    "**Firstly**, how to get things out of a program. And in Python this is done with `print`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a) # will output 1 on a line\n",
    "print(a, name) # will output 1 followed by a space (,) and then bogdan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-terrorist",
   "metadata": {},
   "source": [
    "**Secondly**, how to assign a variable. Variable are names for places in memory where data is stored. Knowing the name is more convenient than knowing the exact address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1           # here \"a\" will point to a memory zone\n",
    "                # that contains the number 1\n",
    "name = 'bogdan' # here \"name\" will point to a memory zone\n",
    "                # that contains the string 'bogdan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-index",
   "metadata": {},
   "source": [
    "**Thirdly**, how to get things in from outside a program. Good programmers always load things from files. Here's how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('bogdan/dobrica.py', 'r') # open the specified file for reading\n",
    "print(fp.read()) # read the whole content of the file\n",
    "fp.close() # don't forget to close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-spanking",
   "metadata": {},
   "source": [
    "**Fourthly**, how to check conditions. This will allow you to make decisions inside a computer program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'bogdan':\n",
    "    print('Oh! Bogdan! I know you!')\n",
    "else:\n",
    "    print('I though your name is Bogdan!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-destiny",
   "metadata": {},
   "source": [
    "And last, **fifthly**, how to loop through things. While usually you would use a `for` for this, `while` is way more flexible. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "while a < 10:\n",
    "    print(a, 'x', a, '=', a*a)\n",
    "    a = a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-willow",
   "metadata": {},
   "source": [
    "### A few things about lists ###\n",
    "\n",
    "Lists are cool cause you can store lots of things, neatly packed and point to that with a single name. In python, you can use `[]` to define a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [] # this is an empty list\n",
    "b_list = [1,2,3] # this is a list of numbers\n",
    "c_list = ['andrei', 'bogdan', 'cristi'] # this is a list of strings\n",
    "d_list = [1, 2, 'bogdan'] # this is a mixed list\n",
    "e_list = [a_list, b_list, c_list] # this is a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-halloween",
   "metadata": {},
   "source": [
    "One of the cool things with lists is that you can `splice` them. This means you can access just part of the list very easy. Here are some nifty examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[0:2] # get the elements between the first (0)\n",
    "            # to but not including the third (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[1:] # get the elements between the second (1)\n",
    "           # to the last one (empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[:2] # get the elements between the first one (empty)\n",
    "           # to the third one (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[::2] # get the elements between the first one (empty)\n",
    "            # to the last one (empty), but going from 2 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[:-1] # get the elements between the first one (empty)\n",
    "            # to the first from last (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[::-1] # get the elements between the first one (empty)\n",
    "             # to the last one (empty), but with counting backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-accessory",
   "metadata": {},
   "source": [
    "You can change an element of a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[0] = 'dan' # change the last element to 'dan'\n",
    "c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list[0:2] = ['a', 'b'] # or do it for first two elements at a time\n",
    "c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-going",
   "metadata": {},
   "source": [
    "You can append new elements to a list with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list.append('dan')\n",
    "c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-conviction",
   "metadata": {},
   "source": [
    "Or remove an element from a list with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(c_list[0])\n",
    "c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-clear",
   "metadata": {},
   "source": [
    "You can merge two lists, using `+`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = b_list + c_list\n",
    "f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-vegetarian",
   "metadata": {},
   "source": [
    "And multiply lists with numbers like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list = c_list * 3\n",
    "g_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-rochester",
   "metadata": {},
   "source": [
    "Also, you can unpack lists like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = b_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-address",
   "metadata": {},
   "source": [
    "### Open CV Basics ###\n",
    "\n",
    "Install the required libraries for the current kernel:\n",
    "- **opencv-contrib-python==4.1.0.25**, a library that allows capturing and processing of images; this is the only version that installs correctly on Raspian, the OS running on Raspberry PI;\n",
    "- **matplotlib**, a library that allows displaying charts or images;\n",
    "After the first run of the below cell, you have to restart the kernel and run it again.\n",
    "\n",
    "You need to restart the Kernel for this to work so press double-0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install opencv-contrib-python==4.1.0.25\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-champagne",
   "metadata": {},
   "source": [
    "To check that everything worked ok, first let's try to get an image from the camera. For this, we need to load the `cv2` module (actually, the short-lazy name for `opencv-contrib-python`) - which allows us to capture and process images from the camera and the `matplotlib.pyplot` module which allows displaying images.\n",
    "\n",
    "Remember from last-time, that we used `cv2.VideoCapture(0)` to initialize the camera and all the things that allows us to capture an image from the webcam and we used the `read()` function on the obtained object to read a status and a frame in BGR format (meaning, pixels are stored as blue-green-red order).\n",
    "\n",
    "Because images are usually processed as red-green-blue, we use the OpenCV `cvtColor` function which converts an image from a color format to another. We'll use `cv2.COLOR_BGR2RGB` to convert from blue-green-red to red-green-blue.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # loading the opencv module to allow video capturing and image processing\n",
    "import matplotlib.pyplot as plt # loading the module that displays charts or images\n",
    "\n",
    "video = cv2.VideoCapture(0) # initialize the Raspberry Pi Camera\n",
    "success, frame = video.read() # read a frame from the camera\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert the frame to RGB\n",
    "plt.imshow(frame_rgb) # display the image\n",
    "video.release() # release the Raspberry Pi Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-daily",
   "metadata": {},
   "source": [
    "You should run the above cell until you get a nice picture of your face for the next steps. To save that picture as I'd like to show how you can play with it, I'll make a copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_copy = frame_rgb.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-ambassador",
   "metadata": {},
   "source": [
    "Here's how to get the image size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-cattle",
   "metadata": {},
   "source": [
    "Now, you can think of this image as a 3D list. You have width, you have height but you also have depth - that is, color depth, red-green-blue. So all things from lists apply. For example, you can fill with white a square in the top-left corner like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_copy[0:250,0:250,:] = [ 5, 227, 235 ] # red = 255, green = 255, blue = 255\n",
    "plt.imshow(frame_copy) # let's display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-albania",
   "metadata": {},
   "source": [
    "If you need to see a larger image, you can change the default size of the displayed images using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 10) # here, the width is 20 and the height is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-ivory",
   "metadata": {},
   "source": [
    "After running the above command, the displayed image size will change every time `plt.imshow` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-distribution",
   "metadata": {},
   "source": [
    "To put a square in the bottom-left corner - I choosed this one as it's a little harder, is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_copy[-250:,0:250,:] = [ 255, 0, 255 ] # red = 255, green = 255, blue = 255\n",
    "plt.imshow(frame_copy) # let's display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-merit",
   "metadata": {},
   "source": [
    "### Viewing the live feed ###\n",
    "\n",
    "This is a cool thing I though I'd show and it allows me to demonstrate some other modules from Python.\n",
    "\n",
    "But first, let's talk a little about **exceptions**. So, when something bad happens in code, an exception is said to be thrown. The simples example is dividing something to zero. Let's see the exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 0\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-daily",
   "metadata": {},
   "source": [
    "Sometimes is not cool to have this kind of errors. So we can **catch** them and tackle them in code. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    c = a / b\n",
    "except:\n",
    "    print('Are you nuts?! You can\\'t divide', a, 'to', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-dublin",
   "metadata": {},
   "source": [
    "So why did I talked about this exceptions? Well, remember **CTRL+C** that stops a program when running? The same thing happens when in a running cell you press **stop**. So this will generate an exception that we can catch and exit gracefully from the program.\n",
    "\n",
    "We need this, cause if we want to display a live feed from the camera, we need an infinite loop that we can interrupt by pressing **CTRL+C**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def functie():\n",
    "    return ('numar',True,3)\n",
    "\n",
    "_, b, _ = functie()\n",
    "print('b', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-category",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time # load the time module; i'll use it here only for the sleep function\n",
    "import IPython # load the IPython modules; this gives access to low level Jupyter Notebook functions\n",
    "\n",
    "video = cv2.VideoCapture(0) # initialize the video capturing device\n",
    "try:\n",
    "    success = True # use this variable to check if the frame was captured successfully\n",
    "    while success: # loop until the device cannot capture a frame\n",
    "        success, frame = video.read() # read a frame\n",
    "        _, jpeg_image = cv2.imencode('.jpeg', frame) # encodes an image into a memory buffer\n",
    "        raw_image = IPython.display.Image(data = jpeg_image) # creates an IPython image given the raw data\n",
    "        IPython.display.clear_output(True) # clears the output of this cell, waiting until other data is available\n",
    "        IPython.display.display(raw_image) # display the IPython image into the notebook\n",
    "        time.sleep(0.2) # wait 0.2 seconds\n",
    "except KeyboardInterrupt: # if stop was pressed (or CTRL+C in the console)\n",
    "    pass # do nothing\n",
    "finally: # but anyway, if it was pressed or not\n",
    "    video.release() # release the video capturing device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-percentage",
   "metadata": {},
   "source": [
    "### Let's do some face detection ###\n",
    "\n",
    "Doing face detection requires a machine learning model. This one that I'll use is called [Haar Cascade Classifier](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) and is almost embeded in the OpenCV. I say almost embedded as you need to download the model from their GitHub repository. Luckly we can run commands in the cells.\n",
    "\n",
    "Download the machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-instrumentation",
   "metadata": {},
   "source": [
    "This used the **wget** command which comes from **world wide web get** and which downloads things from the internet.\n",
    "\n",
    "Now, I need to load the downloaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') # load the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-progress",
   "metadata": {},
   "source": [
    "The classifier can be invoked on a gray-scale image using the `detectMultiScale` method, which for convenience I'll put in a wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert the frame in gray-scale\n",
    "    faces = face_cascade.detectMultiScale( # the face detector\n",
    "        gray, # gray-scale image\n",
    "        scaleFactor = 1.1, # how much the image size is reduced at each scale\n",
    "        minSize = (30, 30) # the minimum size of a detected object\n",
    "    )\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-costa",
   "metadata": {},
   "source": [
    "And now I can use it simple, like below and will return a list of rectangles bounding all the faces in a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = detect_faces(frame_rgb) # get the list of rectangles bounding faces\n",
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in faces:\n",
    "    print(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-summit",
   "metadata": {},
   "source": [
    "Let's put a rectangle over the detected faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = faces[0] # unwrap the list\n",
    "frame_rgb_copy = frame_rgb.copy() # make a copy of the image\n",
    "cv2.rectangle(frame_rgb_copy, (x, y), (x+w, y+h), (0, 255, 0), 2) # draw the rectangle\n",
    "plt.imshow(frame_rgb_copy) # display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-porter",
   "metadata": {},
   "source": [
    "Now, the trick is to make it detect faces continuously and for that I'll just merge the two pieces of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # load the time module; i'll use it here only for the sleep function\n",
    "import IPython # load the IPython modules; this gives access to low level Jupyter Notebook functions\n",
    "\n",
    "video = cv2.VideoCapture(0) # initialize the video capturing device\n",
    "try:\n",
    "    success = True # use this variable to check if the frame was captured successfully\n",
    "    while success: # loop until the device cannot capture a frame\n",
    "        success, frame = video.read() # read a frame\n",
    "        faces = detect_faces(frame)\n",
    "        if list(faces):\n",
    "            for x, y, w, h in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        _, jpeg_image = cv2.imencode('.jpeg', frame) # encodes an image into a memory buffer\n",
    "        raw_image = IPython.display.Image(data = jpeg_image) # creates an IPython image given the raw data\n",
    "        IPython.display.clear_output(True) # clears the output of this cell, waiting until other data is available\n",
    "        IPython.display.display(raw_image) # display the IPython image into the notebook\n",
    "        time.sleep(0.2) # wait 0.2 seconds\n",
    "except KeyboardInterrupt: # if stop was pressed (or CTRL+C in the console)\n",
    "    pass # do nothing\n",
    "finally: # but anyway, if it was pressed or not\n",
    "    video.release() # release the video capturing device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-colors",
   "metadata": {},
   "source": [
    "As you saw in the above example, the Haar Cascade algorithm used default by OpenCV is not the most accurate one - although, is by far the fastest and easy to implement in most mobile devices. An improvement to this algorithm is to use the [Histogram of Oriented Gradients (HOG)](http://dlib.net/fhog_object_detector_ex.cpp.html) algorithm implemented by the `dlib` module. The performance penalty is about 80% over the HOG, but the results are way better and it comes with a useful trick that'll be put to use a few cells down.\n",
    "\n",
    "First, let's replace the `detect_faces` function to use HOG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def detect_faces(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert the frame in gray-scale\n",
    "    faces = detector(gray)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-inspector",
   "metadata": {},
   "source": [
    "To run the face detection, just apply the function to a frame. This time, the detection will return a list of `dlib` rectangles, which are objects that have `.top()`, `.bottom()`, `.left()` and `.right()` methods to extract the top-left and bottom-right coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect_faces(frame)\n",
    "if result: # remember, result is a list of rectangles and this is how to test if it is not empty\n",
    "    print(\n",
    "        'left:', result[0].left(),\n",
    "        'top:', result[0].top(),\n",
    "        'right:', result[0].right(),\n",
    "        'bottom:', result[0].bottom()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-proof",
   "metadata": {},
   "source": [
    "As I said, `dlib` is way cooler than a simple face detector. It has a model for detecting facial landmarks that we can download directly from this repository. It will detect [68 facial landmarks](https://towardsdatascience.com/facial-mapping-landmarks-with-dlib-python-160abcf7d672)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/italojs/facial-landmarks-recognition/raw/master/shape_predictor_68_face_landmarks.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-framing",
   "metadata": {},
   "source": [
    "Here's how to load the downloaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-mustang",
   "metadata": {},
   "source": [
    "And as usual, let's wrap the thing into a function, for ease of use. The function will return a list of tuples, with a face rectangle and a list of 68 point objects - that have `.x` and `.y` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(frame):\n",
    "    detected = []\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    for face in detect_faces(frame):\n",
    "        landmarks = predictor(image=gray, box=face)\n",
    "        detected.append((face, landmarks))\n",
    "    return detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-dependence",
   "metadata": {},
   "source": [
    "Here's how the result looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_landmarks = detect_landmarks(frame)\n",
    "detected_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-civilization",
   "metadata": {},
   "source": [
    "And, to better see there results, here's how to draw them on the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2RGB)\n",
    "for face, landmarks in detected_landmarks:\n",
    "    cv2.rectangle(output_image, (face.left(), face.top()), (face.right(), face.bottom()), (255, 0, 0), 2)\n",
    "    for landmark in landmarks.parts():\n",
    "        cv2.circle(output_image, (landmark.x, landmark.y), 2, (0, 255, 0), 2)\n",
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-moral",
   "metadata": {},
   "source": [
    "I'll download next a glasses image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ublo.ro/wp-content/uploads/2021/03/glasses.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-airline",
   "metadata": {},
   "source": [
    "Now, I'll load it as an OpenCV frame. The function for this is `cv2.imread` which takes as the first parameter the path to the file. Luckly, when I downloaded the file it is in the current directory. You'll notice a second parameter, `cv2.IMREAD_UNCHANGED`. This is present because glasses is a transparent image and it has 4 channels, instead of 3. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses = cv2.imread('glasses.png', cv2.IMREAD_UNCHANGED)\n",
    "glasses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-allah",
   "metadata": {},
   "source": [
    "The fourth channel is actually a transparency mask: if the value for the 4th channel is 255, then that pixel is opaque. If the value for the 4th channel is 0, then the pixel is completely transparent. Any value between those represents the degree of transparency. Here's how the mask looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(glasses[:,:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-strengthening",
   "metadata": {},
   "source": [
    "Remember that the glasses frame is black? That's `(0, 0, 0)` in BGR components. But the other pixels are `(0, 0, 0)` as well, as they actually don't matter as they are transparent. So here's how the image looks like, without the transparency information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(glasses[:,:,:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-presence",
   "metadata": {},
   "source": [
    "I'll read the width and height of the image, to make it easy for me next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses_h, glasses_w, _ = glasses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-promotion",
   "metadata": {},
   "source": [
    "Also, I'll extract the alpha channel information and will create a 3-channels image from it, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # I need the matrices modules\n",
    "alpha = glasses[:, :, 3] # take only the 4th channel\n",
    "alpha = np.expand_dims(alpha, axis = 2) # alpha is a 2-D matrix; but I want a 3-D one so I'll add a dummy dimension\n",
    "alpha = alpha / 255.0 # I'd like for every pixel to be between 0 and 1 instead of 0 and 255\n",
    "alpha = np.concatenate([alpha, alpha, alpha], axis = 2) # and will create 3 copies of alpha for each channel\n",
    "inv_alpha = 1 - alpha # also, it's a good idea to have an inverted alpha: where 1 is 0 and 0 is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-iceland",
   "metadata": {},
   "source": [
    "Now, I'll use this alpha to superimpose the two images. I'll do the trick that I've used above, to modify just part of the original image, to be more specific the top-left rectangle that has the same size as the glasses image, like this:\n",
    "- I'll multipy each pixel from the original image with `inv_alpha`. So, where glasses are transparent 0, `inv_alpha` is 1 that the actual pixel color will not change;\n",
    "- I'll multiply each glasses pixel with `alpha`. So, where glasses are opaque, `alpha` is actually 1 so multiplying that pixel with 1 will not change color, but multiplying with 0 the other pixels, will make them irrelevant;\n",
    "- I'll simply add the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_with_glasses = frame.copy() # make first a copy of the frame\n",
    "# change only the top-left rectangle of (glasses_w, glasses_h) size\n",
    "# multiply the original frame with inv_alpha and add\n",
    "# the glasses image multiplied with alpha\n",
    "frame_with_glasses[:glasses_h,:glasses_w,:] = \\\n",
    "    frame[:glasses_h,:glasses_w,:] * inv_alpha + \\\n",
    "    glasses[:,:,:3] * alpha\n",
    "plt.imshow(frame_with_glasses) # display the superimposed images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-brisbane",
   "metadata": {},
   "source": [
    "I'll now mark the center of the glasses lenses as I'll have to match the center of the eyes there. These values are obtained by trial and error or you can use a graphics software, like [GIMP](https://www.gimp.org) to determine the coordinates using a mouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses_left_eye = dlib.point(130, 120) # this is the left eye center; I could've used tuples, but I like the points =)\n",
    "glasses_right_eye = dlib.point(380, 120) # this is the right eye center\n",
    "glasses_copy = glasses[:,:,3].copy() # make a copy of the 4th channel of the image\n",
    "cv2.circle(glasses_copy, (glasses_left_eye.x, glasses_left_eye.y), 10, (255,), 2) # draw a circle for the left eye\n",
    "cv2.circle(glasses_copy, (glasses_right_eye.x, glasses_left_eye.y), 10, (255,), 2) # draw a circle for the right eye\n",
    "plt.imshow(glasses_copy) # show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-inspiration",
   "metadata": {},
   "source": [
    "For the eyes, I'll take all the landmarks that represents an eye and will take the average of the coordinates. Geometry tells me that this is how I'll obtain the center of gravity for the eye (the exact center). I'm using `numpy` ability to do mathematical operations on any kind of matrices, just because I'm lazy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_left_eye = np.mean(np.array(detected_landmarks[0][1].parts()[36:42]))\n",
    "face_right_eye = np.mean(np.array(detected_landmarks[0][1].parts()[42:48]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-mechanism",
   "metadata": {},
   "source": [
    "I'm defining a distance function. This is the Euclidean distance between two `dlib.point` objects. It's here to allow me to write less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    \"\"\"\n",
    "    Function that computes the euclidean distance between two dlib.dpoint\n",
    "    @param a (dlib.dpoint):\n",
    "    @param b (dlib.dpoint):\n",
    "    @return (np.float32): the euclidean distance between a and b\n",
    "    \"\"\"\n",
    "    return np.sqrt((a.x - b.x) * (a.x - b.x) + (a.y - b.y) * (a.y - b.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-muslim",
   "metadata": {},
   "source": [
    "The first trick to make the glasses really blend with the image is to have them the correct size. To find that out, the distance between the center of the eyes on the glasses should be the same with the distance of the center of the eyes on the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = distance(face_left_eye, face_right_eye) / distance(glasses_left_eye, glasses_right_eye) # compute the scaling ratio\n",
    "glasses_shrinked_w = int(ratio * glasses_w) # so the new glasses width is the old width times the ratio\n",
    "glasses_shrinked_h = int(ratio * glasses_h) # same for height\n",
    "glasses_shrinked = cv2.resize(glasses, (glasses_shrinked_w, glasses_shrinked_h)) # and let's resize\n",
    "# remember that this is done on the 4-channel image. see?\n",
    "glasses_shrinked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-adams",
   "metadata": {},
   "source": [
    "Let's see that the alpha channel is still there and that I can superimpose the two images, like I did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.expand_dims(glasses_shrinked[:, :, 3], axis = 2) / 255.0\n",
    "alpha = np.concatenate([alpha, alpha, alpha], axis = 2)\n",
    "inv_alpha = 1.0 - alpha\n",
    "frame_with_glasses = frame.copy()\n",
    "frame_with_glasses[:glasses_shrinked_h,:glasses_shrinked_w,:] = frame[:glasses_shrinked_h,:glasses_shrinked_w,:] * inv_alpha + glasses_shrinked[:,:,:3] * alpha\n",
    "plt.imshow(frame_with_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-stephen",
   "metadata": {},
   "source": [
    "Uhuu! Next trick for making the glasses match, is to move them on the correct position. To do this, I'll actually move the glasses left eye center to match the face left eye center:\n",
    "- because the distance between the eyes is the same for both images, I just need to see how far is the center of the eye for the face compared to the top-left corner;\n",
    "- then, I'll substract the distance on each axis for the center of the left eye compared to the glasses;\n",
    "- use the new offset values to alter a rectangle that has the new top-left coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, substract from the center of face-eye the center of glasses-eye\n",
    "offset_x = int(face_left_eye.x - ratio * glasses_left_eye.x)\n",
    "offset_y = int(face_left_eye.y - ratio * glasses_right_eye.y)\n",
    "# add the offset to the width and height, just to write less\n",
    "offset_w = glasses_shrinked_w + offset_x\n",
    "offset_h = glasses_shrinked_h + offset_y\n",
    "# make a copy of the frame\n",
    "frame_with_glasses = frame.copy()\n",
    "# here I'm doing the alpha merging\n",
    "frame_with_glasses[offset_y:offset_h,offset_x:offset_w,:] = \\\n",
    "    frame[offset_y:offset_h,offset_x:offset_w,:] * inv_alpha + \\\n",
    "    glasses_shrinked[:,:,:3] * alpha\n",
    "plt.imshow(frame_with_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-trigger",
   "metadata": {},
   "source": [
    "The third trick is to take into consideration is to compensate for my head movement. That's no easy feat, but OpenCV to the rescue.\n",
    "- OpenCV has a function that [affinely](https://en.wikipedia.org/wiki/Affine_geometry) transforms a [quadrilateral](https://en.wikipedia.org/wiki/Quadrilateral) - [`warpPerspective`](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87);\n",
    "- to use this function, I need to construct a quadrilateral from the landmarks that are given to me by the model and match it to a quadrilateral to match a **normal** image - when the face is oriented perfectly;\n",
    "\n",
    "Here's a schematic of the solution:\n",
    "![warpAffine](https://ublo.ro/wp-content/uploads/2021/03/glasses-explained.png)\n",
    "- I'll get the center of the top landmarks (L1, R1) and bottom landmarks (L2, R2);\n",
    "- I'll compute the center between the centers of the eyes; this will be my normal reference center and will match the center on the moved head;\n",
    "- from the computed center, I'll move left and right from it with half the distance between the center of the eyes and get the normal left and right eye centers;\n",
    "- from the normal centers I'll move up and down with the average distance between (L1, L2) and (R1, R2) and will obtain the normal_L1, normal_L2, normal_R1, normal_R2;\n",
    "- given the real and the normal points, I'll use the [getPerspectiveTransform](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga8c1ae0e3589a9d77fffc962c49b22043) to compute the transformation matrix;\n",
    "- I'll put the glasses on an image the same size as the frame (warping works on the whole image);\n",
    "- and will warp the glasses image and merge it with the original frame.\n",
    "\n",
    "Let me define a function that computes the center of a segment given two points.\n",
    "\n",
    "**!Warning!** Be very careful not mixing the `dlib.point` with `dlib.dpoint`. The last one has floating point numbers, while the first one has only integer coordinates. Sometimes integer coordinates are desired - especially when drawing on the images - but not when we do matrix computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(a, b):\n",
    "    \"\"\"\n",
    "    This function takes two dlib.dpoint and computes the average (center)\n",
    "    @param a (dlib.dpoint):\n",
    "    @param b (dlib.dpoint):\n",
    "    @return (dlib.dpoint): the middle of the segment formed by a and b.\n",
    "    \"\"\"\n",
    "    return dlib.dpoint(np.array([\n",
    "        0.5 * (a.x + b.x),\n",
    "        0.5 * (a.y + b.y)\n",
    "    ], dtype = np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-finder",
   "metadata": {},
   "source": [
    "I'll now compute the L1, L2, R1, R2 and center points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = np.mean(np.array(detected_landmarks[0][1].parts()[37:39]))\n",
    "L2 = np.mean(np.array(detected_landmarks[0][1].parts()[40:42]))\n",
    "R1 = np.mean(np.array(detected_landmarks[0][1].parts()[43:45]))\n",
    "R2 = np.mean(np.array(detected_landmarks[0][1].parts()[46:48]))\n",
    "C = center(face_left_eye, face_right_eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-requirement",
   "metadata": {},
   "source": [
    "Computing the average distance, but make it relative to the distance between the center of the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_between_eyes = distance(face_left_eye, face_right_eye)\n",
    "eye_height_to_distance = \\\n",
    "    ( \\\n",
    "        distance(L1, L2) + \\\n",
    "        distance(R1, R2)\n",
    "    ) / (2.0 * distance_between_eyes)\n",
    "eye_height_to_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-innocent",
   "metadata": {},
   "source": [
    "I'll add a new helper function, to translate a point either on the x or y axis, or both. This will help me construct my normal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(point, x = None, y = None):\n",
    "    \"\"\"\n",
    "    This function translates a dlib.dpoint with either x or y and returns the result.\n",
    "    @param point (dlib.dpoint):\n",
    "    @param x (float): the amount to translate on the x-axis (optional), no x translation if missing;\n",
    "    @param y (float): the amount to translate on the y-axis (optional), no y translation if missing;\n",
    "    @return (dlib.dpoint): the translated point\n",
    "    \"\"\"\n",
    "    x = x or 0\n",
    "    y = y or 0\n",
    "    return dlib.dpoint(np.array([point.x + x, point.y + y], dtype = np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-graduation",
   "metadata": {},
   "source": [
    "As I said, the normal left eye center is the translation on the x axis (horizontally) for the center. Same for the normal right eye center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_left_eye = translate(C, x = -distance_between_eyes / 2)\n",
    "normal_right_eye = translate(C, x = distance_between_eyes / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-extreme",
   "metadata": {},
   "source": [
    "Now, to compute the normal L and normal R points, I start from the center of the eyes and move up and down on the y axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_y = 0.5 * eye_height_to_distance * distance_between_eyes # compute how much to move up and down\n",
    "normal_L1 = translate(normal_left_eye, y = -translate_y) # move up, from the left eye\n",
    "normal_L2 = translate(normal_left_eye, y = translate_y) # move down, from the left eye\n",
    "normal_R1 = translate(normal_right_eye, y = -translate_y) # move up, from the right eye\n",
    "normal_R2 = translate(normal_right_eye, y = translate_y) # move down, from the right eye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-farmer",
   "metadata": {},
   "source": [
    "As drawing things always need tuples of integers and my points usually are floating point numbers, I define a function to easily convert to tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_tuple(point):\n",
    "    \"\"\"\n",
    "    This function converts a dlib.point into a tuple of integers.\n",
    "    @param point (dlib.point):\n",
    "    @return (tuple(int, int)): A tuple with x and y coordinates converted to integers.\n",
    "    \"\"\"\n",
    "    return (int(point.x), int(point.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-debut",
   "metadata": {},
   "source": [
    "So, now that I have everything I need, I'll compute the transformation matrix that goes from normal points to the actual points. This will show me how to convert an ideal position to a real one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = cv2.getPerspectiveTransform(\n",
    "    np.array([\n",
    "        [normal_L1.x, normal_L1.y],\n",
    "        [normal_L2.x, normal_L2.y],\n",
    "        [normal_R2.x, normal_R2.y],\n",
    "        [normal_R1.x, normal_R1.y]\n",
    "    ], dtype = np.float32),\n",
    "    np.array([\n",
    "        [L1.x, L1.y],\n",
    "        [L2.x, L2.y],\n",
    "        [R2.x, R2.y],\n",
    "        [R1.x, R1.y]\n",
    "    ], dtype = np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-millennium",
   "metadata": {},
   "source": [
    "I'll prepare the image for warping - this means creating a new 4-channel image the same size as the frame and add the shrinked glasses image to it. I'll consider placing it with the same trick as before, but not from the left eye center but from the center between the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses_warped = np.zeros((frame.shape[0], frame.shape[1], 4), dtype = np.uint8)\n",
    "glasses_C = center(glasses_left_eye, glasses_right_eye)\n",
    "offset_x = int(C.x - int(ratio * glasses_C.x))\n",
    "offset_y = int(C.y - int(ratio * glasses_C.y))\n",
    "offset_w = offset_x + glasses_shrinked_w\n",
    "offset_h = offset_y + glasses_shrinked_h\n",
    "glasses_warped[offset_y:offset_h, offset_x:offset_w, :] = glasses_shrinked\n",
    "\n",
    "plt.imshow(glasses_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-thumb",
   "metadata": {},
   "source": [
    "And warp that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasses_warped = cv2.warpPerspective(glasses_warped, M, (frame.shape[1], frame.shape[0]))\n",
    "plt.imshow(glasses_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-papua",
   "metadata": {},
   "source": [
    "And now, there's an easy blend between the two - as they are the same size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_with_glasses = frame.copy()\n",
    "alpha = np.expand_dims(glasses_warped[:,:,3], axis = 2) / 255.0\n",
    "alpha = np.concatenate([alpha, alpha, alpha], axis = 2)\n",
    "inv_alpha = 1.0 - alpha\n",
    "frame_with_glasses = frame_with_glasses * inv_alpha + glasses_warped[:,:,:3] * alpha\n",
    "frame_with_glasses = frame_with_glasses.astype(np.uint8) # this is the only thing that's needed, to make the image integer\n",
    "plt.imshow(frame_with_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-colony",
   "metadata": {},
   "source": [
    "I'll create a few functions now from the code above, so we can run things live:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_points(landmarks):\n",
    "    \"\"\"\n",
    "    This function takes a `landmark` object from dlib,\n",
    "    which contains .parts() method and returns the L1, L2,\n",
    "    R1, R2 points in the correct order, as a list.\n",
    "    @param landmarks :\n",
    "    @return list(list, list, list, list): A list of landmark points used for glasses.\n",
    "    \"\"\"\n",
    "    L1 = np.mean(np.array(landmarks.parts()[37:39]))\n",
    "    L2 = np.mean(np.array(landmarks.parts()[40:42]))\n",
    "    R1 = np.mean(np.array(landmarks.parts()[43:45]))\n",
    "    R2 = np.mean(np.array(landmarks.parts()[46:48]))\n",
    "    \n",
    "    return [\n",
    "        [L1.x, L1.y],\n",
    "        [L2.x, L2.y],\n",
    "        [R2.x, R2.y],\n",
    "        [R1.x, R1.y]\n",
    "    ]\n",
    "\n",
    "def get_center(landmarks):\n",
    "    \"\"\"\n",
    "    This function takes a `landmark` object from dlib,\n",
    "    which contains .parts() method and returns the center of the face.\n",
    "    @param landmarks :\n",
    "    @return (dlib.dpint) : the center of the faces.\n",
    "    \"\"\"\n",
    "    face_left_eye = np.mean(np.array(landmarks.parts()[36:42]))\n",
    "    face_right_eye = np.mean(np.array(landmarks.parts()[42:48]))\n",
    "    face_center = center(face_left_eye, face_right_eye)\n",
    "    return face_center\n",
    "\n",
    "def get_distances(real_points):\n",
    "    \"\"\"\n",
    "    This function takes the real points as returned by get_real_points,\n",
    "    and computes the distance between the eyes and the eye height relative\n",
    "    to the distance between the eyes.\n",
    "    @param real_points (list(list, list, list, list)) : the list of real points\n",
    "    @return (tuple(float, float)) : the distance between the eyes and the relative height\n",
    "    \"\"\"\n",
    "    L1 = dlib.dpoint(np.array(real_points[0]))\n",
    "    L2 = dlib.dpoint(np.array(real_points[1]))\n",
    "    R2 = dlib.dpoint(np.array(real_points[2]))\n",
    "    R1 = dlib.dpoint(np.array(real_points[3]))\n",
    "    face_left_eye = center(L1, L2)\n",
    "    face_right_eye = center(R1, R2)\n",
    "\n",
    "    distance_between_the_eyes = distance(face_left_eye, face_right_eye)\n",
    "    eye_height_to_distance = \\\n",
    "        ( \\\n",
    "            distance(L1, L2) + \\\n",
    "            distance(R1, R2)\n",
    "        ) / (2 * distance_between_the_eyes)\n",
    "    \n",
    "    return distance_between_the_eyes, eye_height_to_distance\n",
    "\n",
    "def get_normal_points(face_center, distance_between_the_eyes, eye_height_to_distance):\n",
    "    \"\"\"\n",
    "    This function takes the center of the face, the distance between the eyes and\n",
    "    the relative eye height and computes an ideal placement of the L and R points.\n",
    "    @param face_center (dlib.dpoint) : the center of the face\n",
    "    @param distance_between_the_eyes (float) :\n",
    "    @param eye_height_to_distance (float) : the relative eye height to the distance between the eyes\n",
    "    @return (list(list, list, list, list)) : a list of points in correct order, same as get_real_points\n",
    "    \"\"\"\n",
    "    normal_left_eye = translate(face_center, x = -0.5 * distance_between_the_eyes)\n",
    "    normal_right_eye = translate(face_center, x = 0.5 * distance_between_the_eyes)\n",
    "    translate_y = 0.5 * eye_height_to_distance * distance_between_the_eyes\n",
    "    normal_L1 = translate(normal_left_eye, y = -translate_y)\n",
    "    normal_L2 = translate(normal_left_eye, y = translate_y)\n",
    "    normal_R1 = translate(normal_right_eye, y = -translate_y)\n",
    "    normal_R2 = translate(normal_right_eye, y = translate_y)\n",
    "    return [\n",
    "        [normal_L1.x, normal_L1.y],\n",
    "        [normal_L2.x, normal_L2.y],\n",
    "        [normal_R2.x, normal_R2.y],\n",
    "        [normal_R1.x, normal_R1.y]\n",
    "    ]\n",
    "\n",
    "def prepare_glasses(distance_between_the_eyes):\n",
    "    \"\"\"\n",
    "    This function takes the distance between the eyes and prepares (resizes) the\n",
    "    glasses image. Also, computes the resize ratio and the coordinates of the glasses center.\n",
    "    @param distance_between_the_eyes (float) : the distance between the eyes\n",
    "    @return (tuple(image, float, dlib.dpoint)) : the image, the scale ratio and the center\n",
    "    \"\"\"\n",
    "    glasses_left_eye = dlib.dpoint(130, 120)\n",
    "    glasses_right_eye = dlib.dpoint(380, 120)\n",
    "    glasses_h, glasses_w, _ = glasses.shape\n",
    "    \n",
    "    ratio = distance_between_the_eyes / distance(glasses_left_eye, glasses_right_eye)\n",
    "    glasses_shrinked_w = int(ratio * glasses_w)\n",
    "    glasses_shrinked_h = int(ratio * glasses_h)\n",
    "    glasses_shrinked = cv2.resize(glasses, (glasses_shrinked_w, glasses_shrinked_h))\n",
    "    \n",
    "    glasses_center = center(glasses_left_eye, glasses_right_eye)\n",
    "    \n",
    "    return glasses_shrinked, ratio, glasses_center\n",
    "\n",
    "def warp_glasses(glasses_shrinked, ratio, glasses_center, face_center, M, frame_size):\n",
    "    \"\"\"\n",
    "    This function takes care of resizing the glasses image.\n",
    "    @param glasses_shrinked (image) : the glasses shrinked image, as prepared by prepare_glasses;\n",
    "    @param ratio (float) : the resize ratio\n",
    "    @param glasses_center (dlib.dpoint) : the center of the glasses\n",
    "    @param face_center (dlib.dpoint) : the center of the face\n",
    "    @param M (matrix) : the transformation matrix\n",
    "    @param frame_size (tuple(int, int)) : the width and height of the frame\n",
    "    @return (image) : the prepared and warped image of the glasses\n",
    "    \"\"\"\n",
    "    width, height = frame_size\n",
    "    glasses_warped = np.zeros((height, width, 4), dtype = np.uint8)\n",
    "    \n",
    "    offset_x = int(face_center.x - int(ratio * glasses_center.x))\n",
    "    offset_y = int(face_center.y - int(ratio * glasses_center.y))\n",
    "    offset_w = offset_x + glasses_shrinked.shape[1]\n",
    "    offset_h = offset_y + glasses_shrinked.shape[0]\n",
    "    glasses_warped[offset_y:offset_h, offset_x:offset_w, :] = glasses_shrinked\n",
    "\n",
    "    glasses_warped = cv2.warpPerspective(glasses_warped, M, (width, height))\n",
    "\n",
    "    return glasses_warped\n",
    "\n",
    "def merge_images(frame, glasses):\n",
    "    \"\"\"\n",
    "    This function merges the current frame with the glasses.\n",
    "    @param frame (image) : current frame\n",
    "    @param glasses (image) : the warped glasses image\n",
    "    @return (image) : the composed image\n",
    "    \"\"\"\n",
    "    alpha = np.expand_dims(glasses[:,:,3], axis = 2) / 255.0\n",
    "    alpha = np.concatenate([alpha, alpha, alpha], axis = 2)\n",
    "    inv_alpha = 1.0 - alpha\n",
    "    frame_with_glasses = frame * inv_alpha + glasses[:,:,:3] * alpha\n",
    "    return frame_with_glasses.astype(np.uint8)\n",
    "        \n",
    "def add_glasses(frame, landmarks):\n",
    "    \"\"\"\n",
    "    This function is the orchestration function for the whole process.\n",
    "    It takes the frame and landmars as produced by detect_landmarks and\n",
    "    outputs the frame with glasses.\n",
    "    @param frame (image) : the captured frame\n",
    "    @param landmarks (dlib.landmarks) : the list of landmarks produced by the model\n",
    "    @return (image) : the composed frame with glasses\n",
    "    \"\"\"\n",
    "    height, width, _ = frame.shape\n",
    "    face_center = get_center(landmarks)\n",
    "    real_points = get_real_points(landmarks)\n",
    "    distance_between_the_eyes, eye_height_to_distance = get_distances(real_points)\n",
    "    normal_points = get_normal_points(face_center, distance_between_the_eyes, eye_height_to_distance)\n",
    "    M = cv2.getPerspectiveTransform(\n",
    "        np.array(normal_points, dtype = np.float32),\n",
    "        np.array(real_points, dtype = np.float32))\n",
    "    \n",
    "    # This part is for debug purposes. You can change False to True to see the L, R and center points.\n",
    "    if False:\n",
    "        for point in normal_points:\n",
    "            cv2.circle(frame, (int(point[0]), int(point[1])), 2, (255, 0, 0), 2)\n",
    "        for point in real_points:\n",
    "            cv2.circle(frame, (int(point[0]), int(point[1])), 2, (0, 255, 0), 2)\n",
    "\n",
    "        face_left_eye = np.mean(np.array(landmarks.parts()[36:42]))\n",
    "        cv2.circle(frame, (int(face_left_eye.x), int(face_left_eye.y)), 2, (0, 0, 255), 2)\n",
    "        face_right_eye = np.mean(np.array(landmarks.parts()[42:48]))\n",
    "        cv2.circle(frame, (int(face_right_eye.x), int(face_right_eye.y)), 2, (0, 0, 255), 2)\n",
    "        cv2.circle(frame, (int(face_center.x), int(face_center.y)), 2, (0, 0, 255), 2)\n",
    "\n",
    "    \n",
    "    glasses_shrinked, ratio, glasses_center = prepare_glasses(distance_between_the_eyes)\n",
    "    warped_glasses = warp_glasses(glasses_shrinked, ratio, glasses_center, face_center, M, (width, height))\n",
    "    frame_with_glasses = merge_images(frame, warped_glasses)\n",
    "    return frame_with_glasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-tractor",
   "metadata": {},
   "source": [
    "And make it run in real time, by modifying the above cell with the new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # load the time module; i'll use it here only for the sleep function\n",
    "import IPython # load the IPython modules; this gives access to low level Jupyter Notebook functions\n",
    "\n",
    "video = cv2.VideoCapture(0) # initialize the video capturing device\n",
    "try:\n",
    "    success = True # use this variable to check if the frame was captured successfully\n",
    "    while success: # loop until the device cannot capture a frame\n",
    "        success, frame = video.read() # read a frame\n",
    "        faces = detect_landmarks(frame)\n",
    "        if faces:\n",
    "            for face, landmarks in faces:\n",
    "                frame = add_glasses(frame, landmarks)\n",
    "        _, jpeg_image = cv2.imencode('.jpeg', frame) # encodes an image into a memory buffer\n",
    "        raw_image = IPython.display.Image(data = jpeg_image) # creates an IPython image given the raw data\n",
    "        IPython.display.clear_output(True) # clears the output of this cell, waiting until other data is available\n",
    "        IPython.display.display(raw_image) # display the IPython image into the notebook\n",
    "        time.sleep(0.2) # wait 0.2 seconds\n",
    "except KeyboardInterrupt: # if stop was pressed (or CTRL+C in the console)\n",
    "    pass # do nothing\n",
    "finally: # but anyway, if it was pressed or not\n",
    "    video.release() # release the video capturing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-asthma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-opencv",
   "language": "python",
   "name": "python-opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
